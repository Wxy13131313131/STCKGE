#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import os
import time
import json
from tqdm import tqdm
from collections import defaultdict
from typing import Dict, List, Tuple, Optional
import logging
from ..utils.data_loader import create_data_loader
from ..models.stckge import ContinualModel
logger = logging.getLogger(__name__)


class ContinualTrainer:
    """æŒç»­å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, config, data_loader, device='cuda'):
        self.config = config
        self.data_loader = data_loader
        self.device = device


        self.model = ContinualModel(
            num_entities=data_loader.num_entities,
            num_relations=data_loader.num_relations,
            embed_dim=config.embed_dim,
            margin=config.margin,
            distill_weight=getattr(config, 'distill_weight', 0.5),
            temperature=getattr(config, 'temperature', 4.0),
            device=device
        ).to(device)

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=config.learning_rate,
            weight_decay=getattr(config, 'weight_decay', 1e-5)
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.8, patience=5, verbose=True
        )

        # è®­ç»ƒçŠ¶æ€
        self.best_metrics = {}
        self.training_history = []

        # æ—¶é—´ç»Ÿè®¡
        self.total_train_time = 0.0
        self.total_test_time = 0.0
        self.snapshot_train_times = []
        self.snapshot_test_times = []

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(config.checkpoint_dir, exist_ok=True)
        os.makedirs(config.log_dir, exist_ok=True)

    def train_all_snapshots(self):
        """è®­ç»ƒæ‰€æœ‰å¿«ç…§"""
        logger.info("ğŸš€ å¼€å§‹æŒç»­å­¦ä¹ è®­ç»ƒ...")
        overall_start_time = time.time()

        num_snapshots = len(self.data_loader.snapshots)
        all_test_results = {}

        for snapshot_id in range(num_snapshots):
            logger.info(f"\n{'=' * 60}")
            logger.info(f"å¤„ç†å¿«ç…§ {snapshot_id}/{num_snapshots - 1}")
            logger.info(f"{'=' * 60}")

            # è·å–å¿«ç…§ä¿¡æ¯
            snapshot_info = self._get_snapshot_info(snapshot_id)
            self._log_snapshot_info(snapshot_id, snapshot_info)

            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è®­ç»ƒå¥½çš„æ¨¡å‹
            model_path = os.path.join(self.config.checkpoint_dir, f'best_model_snapshot_{snapshot_id}.pth')
            if os.path.exists(model_path):
                logger.info(f"ğŸ” å‘ç°å·²å­˜åœ¨çš„å¿«ç…§ {snapshot_id} æ¨¡å‹ï¼Œç›´æ¥åŠ è½½è¿›è¡Œè¯„ä¼°...")

                # ç¡®ä¿æ¨¡å‹æ‰©å±•åˆ°åˆé€‚å¤§å°
                if self._should_expand_model(snapshot_id):
                    self._expand_model(snapshot_id)

                # åŠ è½½å·²æœ‰æ¨¡å‹
                self._load_best_model(snapshot_id)

                # è®°å½•æ—¶é—´ï¼ˆè·³è¿‡çš„è®­ç»ƒæ—¶é—´ä¸º0ï¼‰
                self.snapshot_train_times.append(0.0)

                # ç›´æ¥è¯„ä¼°
                test_start_time = time.time()
                test_results = self._evaluate_on_all_test_sets_with_weighted_avg(snapshot_id)
                test_time = time.time() - test_start_time
                self.snapshot_test_times.append(test_time)
                self.total_test_time += test_time

                all_test_results[f'after_snapshot_{snapshot_id}'] = test_results

                # è®°å½•åˆ°å†å²
                self.training_history.append({
                    'snapshot_id': snapshot_id,
                    'training_results': {'skipped': True, 'reason': 'model_exists'},
                    'test_results': test_results,
                    'train_time': 0.0,
                    'test_time': test_time
                })

                logger.info(f"âœ… å¿«ç…§ {snapshot_id} è¯„ä¼°å®Œæˆï¼ˆè·³è¿‡è®­ç»ƒï¼Œæµ‹è¯•è€—æ—¶: {test_time:.2f}sï¼‰")
                continue

            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ‰©å±•æ¨¡å‹
            if self._should_expand_model(snapshot_id):
                self._expand_model(snapshot_id)

            # è®¾ç½®è’¸é¦æ¨¡å‹ï¼ˆä»ç¬¬äºŒä¸ªå¿«ç…§å¼€å§‹ï¼‰
            if snapshot_id > 0:
                self._setup_distillation_model()

            # è®­ç»ƒå½“å‰å¿«ç…§
            train_start_time = time.time()
            snapshot_results = self._train_single_snapshot(snapshot_id)
            train_time = time.time() - train_start_time
            self.snapshot_train_times.append(train_time)
            self.total_train_time += train_time

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            self._save_best_model(snapshot_id)

            # è¯„ä¼°å½“å‰æ¨¡å‹åœ¨æ‰€æœ‰ä¹‹å‰æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½ï¼ˆå¸¦åŠ æƒå¹³å‡ï¼‰
            test_start_time = time.time()
            test_results = self._evaluate_on_all_test_sets_with_weighted_avg(snapshot_id)
            test_time = time.time() - test_start_time
            self.snapshot_test_times.append(test_time)
            self.total_test_time += test_time

            all_test_results[f'after_snapshot_{snapshot_id}'] = test_results

            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append({
                'snapshot_id': snapshot_id,
                'training_results': snapshot_results,
                'test_results': test_results,
                'train_time': train_time,
                'test_time': test_time
            })

            logger.info(f"âœ… å¿«ç…§ {snapshot_id} è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼ˆè®­ç»ƒ: {train_time:.2f}s, æµ‹è¯•: {test_time:.2f}sï¼‰")

        # è®¡ç®—æ€»æ—¶é—´
        total_time = time.time() - overall_start_time

        # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆåŒ…å«æ—¶é—´ç»Ÿè®¡ï¼‰
        self._save_final_results(all_test_results, total_time)

        # æ˜¾ç¤ºæ—¶é—´ç»Ÿè®¡
        self._print_time_summary(total_time)

        return all_test_results

    def _get_snapshot_info(self, snapshot_id):
        """è·å–å¿«ç…§ä¿¡æ¯"""
        # è·å–å½“å‰å¿«ç…§çš„å¢é‡ä¿¡æ¯
        incremental_info = self.data_loader.get_incremental_info(snapshot_id)

        # è·å–ä¸‰å…ƒç»„æ•°é‡
        train_dataset = self.data_loader.get_snapshot_dataset(snapshot_id, mode='train')
        valid_dataset = self.data_loader.get_snapshot_dataset(snapshot_id, mode='valid')
        test_dataset = self.data_loader.get_snapshot_dataset(snapshot_id, mode='test')

        return {
            'num_train_triples': len(train_dataset),
            'num_valid_triples': len(valid_dataset),
            'num_test_triples': len(test_dataset),
            'num_new_entities': incremental_info['num_new_entities'],
            'num_new_relations': incremental_info['num_new_relations'],
            'new_entities': incremental_info['new_entities'],
            'old_entities': incremental_info['old_entities'],
            'new_relations': incremental_info['new_relations'],
            'old_relations': incremental_info['old_relations']
        }

    def _log_snapshot_info(self, snapshot_id, info):
        """è®°å½•å¿«ç…§ä¿¡æ¯"""
        logger.info(f"å¿«ç…§ {snapshot_id} ä¿¡æ¯:")
        logger.info(f"  è®­ç»ƒä¸‰å…ƒç»„: {info['num_train_triples']}")
        logger.info(f"  éªŒè¯ä¸‰å…ƒç»„: {info['num_valid_triples']}")
        logger.info(f"  æµ‹è¯•ä¸‰å…ƒç»„: {info['num_test_triples']}")
        logger.info(f"  æ–°å®ä½“æ•°: {info['num_new_entities']}")
        logger.info(f"  æ–°å…³ç³»æ•°: {info['num_new_relations']}")
        logger.info(f"  å½“å‰æ€»å®ä½“æ•°: {self.model.num_entities}")
        logger.info(f"  å½“å‰æ€»å…³ç³»æ•°: {self.model.num_relations}")

    def _should_expand_model(self, snapshot_id):
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰©å±•æ¨¡å‹"""
        if snapshot_id == 0:
            return False

        incremental_info = self.data_loader.get_incremental_info(snapshot_id)
        return (incremental_info['num_new_entities'] > 0 or
                incremental_info['num_new_relations'] > 0)

    def _expand_model(self, snapshot_id):
        """æ‰©å±•æ¨¡å‹ä»¥å®¹çº³æ–°å®ä½“å’Œå…³ç³»"""
        incremental_info = self.data_loader.get_incremental_info(snapshot_id)

        # è®¡ç®—æ–°çš„å®ä½“å’Œå…³ç³»æ•°é‡
        all_entities = set(incremental_info['new_entities']) | set(incremental_info['old_entities'])
        all_relations = set(incremental_info['new_relations']) | set(incremental_info['old_relations'])

        new_num_entities = max(max(all_entities) + 1 if all_entities else 0, self.model.num_entities)
        new_num_relations = max(max(all_relations) + 1 if all_relations else 0, self.model.num_relations)

        if new_num_entities > self.model.num_entities or new_num_relations > self.model.num_relations:
            logger.info(f"æ‰©å±•æ¨¡å‹: å®ä½“ {self.model.num_entities} -> {new_num_entities}, "
                        f"å…³ç³» {self.model.num_relations} -> {new_num_relations}")

            # æ‰©å±•æ¨¡å‹
            self.model.expand_model(new_num_entities, new_num_relations)

            # é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ï¼ˆå‚æ•°æ•°é‡æ”¹å˜äº†ï¼‰
            self.optimizer = optim.Adam(
                self.model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=getattr(self.config, 'weight_decay', 1e-5)
            )

            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='max', factor=0.8, patience=5, verbose=True
            )

    def _setup_distillation_model(self):
        """è®¾ç½®è’¸é¦æ¨¡å‹"""
        previous_model = self.model.copy_model_for_distillation()
        self.model.set_previous_model(previous_model)
        logger.info("è®¾ç½®è’¸é¦æ¨¡å‹å®Œæˆ")

    def _train_single_snapshot(self, snapshot_id):
        """è®­ç»ƒå•ä¸ªå¿«ç…§ï¼ˆä¿®æ­£è¿‡æ‹Ÿåˆé—®é¢˜ï¼‰"""
        # è·å–æ•°æ®åŠ è½½å™¨
        train_loader = self.data_loader.get_snapshot_dataloader(
            snapshot_id,
            mode='train',
            batch_size=self.config.batch_size,
            shuffle=True
        )

        valid_loader = self.data_loader.get_snapshot_dataloader(
            snapshot_id,
            mode='valid',
            batch_size=self.config.batch_size,
            shuffle=False
        )

        # è·å–è’¸é¦ä¿¡æ¯
        incremental_info = self.data_loader.get_incremental_info(snapshot_id)
        old_entities = torch.LongTensor(incremental_info['old_entities']).to(self.device) if incremental_info[
            'old_entities'] else None
        old_relations = torch.LongTensor(incremental_info['old_relations']).to(self.device) if incremental_info[
            'old_relations'] else None

        best_valid_mrr = 0.0
        patience_counter = 0
        epoch_losses = []

        logger.info(f"å¼€å§‹è®­ç»ƒå¿«ç…§ {snapshot_id}ï¼Œå…± {self.config.epochs} ä¸ª epoch")
        logger.info(f"éªŒè¯é¢‘ç‡: æ¯ {getattr(self.config, 'valid_freq', 3)} ä¸ªepochéªŒè¯ä¸€æ¬¡")
        logger.info("=" * 80)
        logger.info(
            f"{'Epoch':<6} {'Loss':<10} {'LR':<12} {'MRR':<8} {'Hit@1':<8} {'Hit@3':<8} {'Hit@10':<8} {'Best':<8}")
        logger.info("=" * 80)

        # åˆ›å»ºepochè¿›åº¦æ¡
        epoch_pbar = tqdm(range(self.config.epochs),
                          desc=f"è®­ç»ƒå¿«ç…§ {snapshot_id}",
                          unit="epoch",
                          position=0,
                          leave=True)

        for epoch in epoch_pbar:
            # è®­ç»ƒä¸€ä¸ªepoch
            epoch_loss = self._train_epoch(
                train_loader, old_entities, old_relations, epoch
            )
            epoch_losses.append(epoch_loss)

            # æŒ‰ç…§è®¾å®šé¢‘ç‡è¿›è¡ŒéªŒè¯
            valid_freq = getattr(self.config, 'valid_freq', 3)
            should_validate = (epoch + 1) % valid_freq == 0 or (epoch + 1) == self.config.epochs

            if should_validate:
                # éªŒè¯
                valid_metrics = self._validate_with_filtering_parallel(valid_loader,snapshot_id)
                current_mrr = valid_metrics['MRR']
                current_lr = self.optimizer.param_groups[0]['lr']

                # æ›´æ–°å­¦ä¹ ç‡
                self.scheduler.step(current_mrr)

                # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                is_best = False
                if current_mrr > best_valid_mrr:
                    best_valid_mrr = current_mrr
                    patience_counter = 0
                    self._save_temp_best_model(snapshot_id)
                    is_best = True
                else:
                    patience_counter += 1

                # æ‰“å°è¯¦ç»†ä¿¡æ¯
                status = "âœ“" if is_best else " "
                logger.info(f"{epoch + 1:<6} {epoch_loss:<10.4f} {current_lr:<12.2e} "
                            f"{valid_metrics['MRR']:<8.4f} {valid_metrics['Hit@1']:<8.4f} "
                            f"{valid_metrics['Hit@3']:<8.4f} {valid_metrics['Hit@10']:<8.4f} "
                            f"{best_valid_mrr:<8.4f} {status}")

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                epoch_pbar.set_postfix({
                    'Loss': f"{epoch_loss:.4f}",
                    'MRR': f"{current_mrr:.4f}",
                    'Best': f"{best_valid_mrr:.4f}",
                    'Pat': f"{patience_counter}/{getattr(self.config, 'patience', 15)}"
                })

                # æ—©åœæ£€æŸ¥
                if patience_counter >= getattr(self.config, 'patience', 15):
                    logger.info(f"æ—©åœåœ¨ epoch {epoch + 1}ï¼Œpatienceè¾¾åˆ° {getattr(self.config, 'patience', 15)}")
                    epoch_pbar.close()
                    break
            else:
                # ä¸éªŒè¯çš„epochï¼Œåªæ˜¾ç¤ºè®­ç»ƒæŸå¤±
                current_lr = self.optimizer.param_groups[0]['lr']
                logger.info(f"{epoch + 1:<6} {epoch_loss:<10.4f} {current_lr:<12.2e} "
                            f"{'---':<8} {'---':<8} {'---':<8} {'---':<8} "
                            f"{best_valid_mrr:<8.4f} ")

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                epoch_pbar.set_postfix({
                    'Loss': f"{epoch_loss:.4f}",
                    'MRR': f"æœªéªŒè¯",
                    'Best': f"{best_valid_mrr:.4f}",
                    'Pat': f"{patience_counter}/{getattr(self.config, 'patience', 15)}"
                })
        else:
            # æ­£å¸¸å®Œæˆæ‰€æœ‰epoch
            epoch_pbar.close()

        logger.info("=" * 80)

        # åŠ è½½æœ€ä½³æ¨¡å‹
        self._load_temp_best_model(snapshot_id)

        # æœ€ç»ˆéªŒè¯
        final_valid_metrics = self._validate_with_filtering_parallel(valid_loader,snapshot_id)
        logger.info(f"âœ… æœ€ç»ˆéªŒè¯ç»“æœ: MRR={final_valid_metrics['MRR']:.4f}, "
                    f"Hit@1={final_valid_metrics['Hit@1']:.4f}, "
                    f"Hit@10={final_valid_metrics['Hit@10']:.4f}")

        return {
            'losses': epoch_losses,
            'best_valid_mrr': best_valid_mrr,
            'final_valid_metrics': final_valid_metrics,
            'total_epochs': len(epoch_losses)
        }

    def _train_epoch(self, train_loader, old_entities, old_relations, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0.0
        num_batches = len(train_loader)

        for batch_idx, (pos_triples, neg_triples) in enumerate(train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            pos_triples = pos_triples.to(self.device)
            if neg_triples.dim() == 3:
                neg_triples = neg_triples.view(-1, 3)
            neg_triples = neg_triples.to(self.device)

            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()

            # å‡†å¤‡è’¸é¦æ ·æœ¬
            sample_triples = None
            if self.model.previous_model is not None and len(pos_triples) > 0:
                sample_size = min(len(pos_triples), 16)
                sample_indices = torch.randperm(len(pos_triples))[:sample_size]
                sample_triples = pos_triples[sample_indices]

            # è®¡ç®—æŸå¤±
            loss_dict = self.model.compute_total_loss(
                pos_triples, neg_triples, old_entities, old_relations, sample_triples
            )

            total_loss_batch = loss_dict['total_loss']

            # åå‘ä¼ æ’­
            total_loss_batch.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), getattr(self.config, 'grad_clip', 1.0))

            # ä¼˜åŒ–å™¨æ­¥éª¤
            self.optimizer.step()

            # æ›´æ–°é‡è¦æ€§æƒé‡
            entities_in_batch = torch.cat([pos_triples[:, 0], pos_triples[:, 2]])
            relations_in_batch = pos_triples[:, 1]
            self.model.current_model.update_importance_weights(entities_in_batch, relations_in_batch)

            total_loss += total_loss_batch.item()

        return total_loss / num_batches

    def _validate_with_filtering_parallel(self, valid_loader, snapshot_id):
        """ä½¿ç”¨è´Ÿè¿‡æ»¤çš„ä¸¥æ ¼éªŒè¯ï¼ˆå¹¶è¡Œæ‰¹å¤„ç†ç‰ˆæœ¬ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ï¼‰"""
        self.model.eval()

        # æ”¶é›†æ‰€æœ‰å·²çŸ¥çš„æ­£æ ·æœ¬ä¸‰å…ƒç»„ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        # ä½¿ç”¨å­—å…¸åŠ é€ŸæŸ¥æ‰¾
        positive_triples_dict = defaultdict(set)
        for i in range(snapshot_id + 1):
            if i < len(self.data_loader.snapshots):
                snapshot_data = self.data_loader.snapshots[i]
                for dataset_name in ['train_triples', 'valid_triples', 'test_triples']:
                    for triple in snapshot_data.get(dataset_name, []):
                        if len(triple) >= 3:
                            h, r, t = triple[0], triple[1], triple[2]
                            positive_triples_dict[(h, r)].add(t)

        total_reciprocal_rank = 0.0
        total_samples = 0
        hit_counts = {1: 0, 3: 0, 10: 0}
        total_mr = 0.0

        # è®¾ç½®è¯„ä¼°æ‰¹å¤§å°
        eval_batch_size = getattr(self.config, 'eval_batch_size', 32)

        with torch.no_grad():
            for pos_triples, _ in valid_loader:
                pos_triples = pos_triples.to(self.device)
                num_triples = pos_triples.shape[0]

                # æ‰¹é‡å¤„ç†æ‰€æœ‰ä¸‰å…ƒç»„
                for batch_start in range(0, num_triples, eval_batch_size):
                    batch_end = min(batch_start + eval_batch_size, num_triples)
                    batch_triples = pos_triples[batch_start:batch_end]

                    # æ‰¹é‡éªŒè¯ç´¢å¼•æœ‰æ•ˆæ€§
                    batch_heads = batch_triples[:, 0]
                    batch_relations = batch_triples[:, 1]
                    batch_tails = batch_triples[:, 2]

                    valid_mask = (
                            (batch_heads < self.model.num_entities) &
                            (batch_relations < self.model.num_relations) &
                            (batch_tails < self.model.num_entities)
                    )

                    if not valid_mask.any():
                        continue

                    # æå–æœ‰æ•ˆä¸‰å…ƒç»„
                    valid_triples = batch_triples[valid_mask]

                    # ä½¿ç”¨IncDEé£æ ¼çš„æ’åè®¡ç®—
                    batch_ranks = self._compute_batch_ranks(
                        valid_triples, positive_triples_dict
                    )

                    # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                    for rank in batch_ranks:
                        total_reciprocal_rank += 1.0 / rank
                        total_mr += rank
                        total_samples += 1

                        for k in [1, 3, 10]:
                            if rank <= k:
                                hit_counts[k] += 1

                # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
                if total_samples >= getattr(self.config, 'max_valid_samples', 1000):
                    break

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        if total_samples == 0:
            return {'MRR': 0.0, 'Hit@1': 0.0, 'Hit@3': 0.0, 'Hit@10': 0.0, 'MR': 0.0}

        metrics = {
            'MRR': total_reciprocal_rank / total_samples,
            'MR': total_mr / total_samples,
            'Hit@1': hit_counts[1] / total_samples,
            'Hit@3': hit_counts[3] / total_samples,
            'Hit@10': hit_counts[10] / total_samples,
            'num_evaluated': total_samples
        }

        return metrics

    def _compute_batch_ranks(self, batch_triples, positive_triples_dict):
        """
        æ’åè®¡ç®—æ–¹æ³•
        """
        batch_size = batch_triples.shape[0]
        batch_heads = batch_triples[:, 0]
        batch_relations = batch_triples[:, 1]
        batch_tails = batch_triples[:, 2]

        # åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„å°¾å®ä½“å€™é€‰
        all_entities = torch.arange(self.model.num_entities, device=self.device)

        # åˆå§‹åŒ–é¢„æµ‹å¾—åˆ†çŸ©é˜µ
        pred = torch.zeros(batch_size, self.model.num_entities, device=self.device)

        # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªä¸‰å…ƒç»„è®¡ç®—å¯¹æ‰€æœ‰å°¾å®ä½“çš„å¾—åˆ†
        for i in range(batch_size):
            h = batch_heads[i]
            r = batch_relations[i]

            # æ‰©å±•å¤´å®ä½“å’Œå…³ç³»ä»¥åŒ¹é…æ‰€æœ‰å€™é€‰å°¾å®ä½“
            heads_expanded = torch.full((self.model.num_entities,), h, device=self.device)
            relations_expanded = torch.full((self.model.num_entities,), r, device=self.device)

            # è®¡ç®—å½“å‰å¤´å®ä½“å’Œå…³ç³»å¯¹æ‰€æœ‰å°¾å®ä½“çš„å¾—åˆ†
            scores = self.model.predict(heads_expanded, relations_expanded, all_entities)
            pred[i] = scores

        # åˆ›å»ºlabelçŸ©é˜µï¼Œæ ‡è®°æ‰€æœ‰å·²çŸ¥çš„æ­£ç¡®tail
        label = torch.zeros(batch_size, self.model.num_entities, device=self.device, dtype=torch.bool)

        for i in range(batch_size):
            h = batch_heads[i].item()
            r = batch_relations[i].item()
            # æ ‡è®°æ‰€æœ‰å·²çŸ¥çš„æ­£ç¡®tail
            if (h, r) in positive_triples_dict:
                for t in positive_triples_dict[(h, r)]:
                    if t < self.model.num_entities:
                        label[i, t] = True

        # æŒ‰ç…§IncDEçš„è¿‡æ»¤æ–¹å¼è¿›è¡Œè¿‡æ»¤
        batch_size_range = torch.arange(batch_size, device=self.device)
        target_pred = pred[batch_size_range, batch_tails]  # å–å‡ºå½“å‰ä¸‰å…ƒç»„ä¸­tailçš„å¾—åˆ†

        # å°†æ‰€æœ‰å…¶ä»–å·²çŸ¥æ­£ç¡®tailçš„å¾—åˆ†è®¾ä¸ºè´Ÿæ— ç©·
        pred = torch.where(label, -torch.ones_like(pred) * 10000000, pred)

        # æ¢å¤å½“å‰ä¸‰å…ƒç»„ä¸­tailçš„å¾—åˆ†
        pred[batch_size_range, batch_tails] = target_pred

        # æ’åè®¡ç®—
        ranks = 1 + torch.argsort(torch.argsort(pred, dim=1, descending=True), dim=1, descending=False)[
            batch_size_range, batch_tails]

        return ranks.cpu().numpy().tolist()

    def _create_label_matrix_for_filtering(self, batch_heads, batch_relations, positive_triples_dict):
        """
        åˆ›å»ºç”¨äºè¿‡æ»¤çš„labelçŸ©é˜µ
        """
        batch_size = len(batch_heads)
        label = torch.zeros(batch_size, self.model.num_entities, device=self.device, dtype=torch.bool)

        for i in range(batch_size):
            h = batch_heads[i].item()
            r = batch_relations[i].item()

            # æ ‡è®°æ‰€æœ‰å·²çŸ¥çš„æ­£ç¡®tail
            if (h, r) in positive_triples_dict:
                for t in positive_triples_dict[(h, r)]:
                    if t < self.model.num_entities:
                        label[i, t] = True

        return label

    def _validate_with_filtering(self, valid_loader, snapshot_id):
        """ä½¿ç”¨è´Ÿè¿‡æ»¤çš„ä¸¥æ ¼éªŒè¯ï¼ˆæ‰¹é‡å¤„ç†ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        self.model.eval()

        # æ”¶é›†æ‰€æœ‰å·²çŸ¥çš„æ­£æ ·æœ¬ä¸‰å…ƒç»„ï¼ˆç”¨äºè¿‡æ»¤ï¼‰
        all_positive_triples = set()
        for i in range(snapshot_id + 1):
            if i < len(self.data_loader.snapshots):
                snapshot_data = self.data_loader.snapshots[i]
                for triple in snapshot_data.get('train_triples', []):
                    if len(triple) >= 3:
                        all_positive_triples.add((triple[0], triple[1], triple[2]))
                for triple in snapshot_data.get('valid_triples', []):
                    if len(triple) >= 3:
                        all_positive_triples.add((triple[0], triple[1], triple[2]))
                for triple in snapshot_data.get('test_triples', []):
                    if len(triple) >= 3:
                        all_positive_triples.add((triple[0], triple[1], triple[2]))

        total_reciprocal_rank = 0.0
        total_samples = 0
        hit_counts = {1: 0, 3: 0, 10: 0}

        # è®¾ç½®è¯„ä¼°æ‰¹å¤§å°
        eval_batch_size = getattr(self.config, 'eval_batch_size', 16)

        with torch.no_grad():
            for pos_triples, _ in valid_loader:
                pos_triples = pos_triples.to(self.device)

                # åˆ†æ‰¹å¤„ç†å½“å‰æ‰¹æ¬¡çš„ä¸‰å…ƒç»„
                num_triples = pos_triples.shape[0]

                for batch_start in range(0, num_triples, eval_batch_size):
                    batch_end = min(batch_start + eval_batch_size, num_triples)
                    batch_triples = pos_triples[batch_start:batch_end]
                    batch_size = batch_triples.shape[0]

                    # æå–æ‰¹æ¬¡ä¸­çš„å¤´å®ä½“ã€å…³ç³»å’Œå°¾å®ä½“
                    batch_heads = batch_triples[:, 0]
                    batch_relations = batch_triples[:, 1]
                    batch_tails = batch_triples[:, 2]

                    # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
                    valid_mask = (
                            (batch_heads < self.model.num_entities) &
                            (batch_relations < self.model.num_relations) &
                            (batch_tails < self.model.num_entities)
                    )

                    if not valid_mask.any():
                        continue

                    # åªå¤„ç†æœ‰æ•ˆçš„ä¸‰å…ƒç»„
                    valid_indices = valid_mask.nonzero(as_tuple=True)[0]
                    valid_heads = batch_heads[valid_indices]
                    valid_relations = batch_relations[valid_indices]
                    valid_tails = batch_tails[valid_indices]
                    valid_batch_size = len(valid_indices)

                    # ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªä¸‰å…ƒç»„è®¡ç®—æ’å
                    for idx in range(valid_batch_size):
                        h = valid_heads[idx].item()
                        r = valid_relations[idx].item()
                        t = valid_tails[idx].item()

                        # æ‰¹é‡è®¡ç®—æ‰€æœ‰å€™é€‰å®ä½“çš„å¾—åˆ†
                        # æ–¹æ³•1ï¼šå¯¹æ‰€æœ‰å®ä½“è¿›è¡Œè¯„åˆ†ï¼ˆå†…å­˜å‹å¥½ä½†è®¡ç®—å¯†é›†ï¼‰
                        if self.model.num_entities <= 10000:  # å®ä½“æ•°é‡è¾ƒå°‘æ—¶
                            all_entities = torch.arange(self.model.num_entities, device=self.device)
                            heads = torch.full((self.model.num_entities,), h, device=self.device)
                            relations = torch.full((self.model.num_entities,), r, device=self.device)

                            # æ‰¹é‡è®¡ç®—å¾—åˆ†
                            scores = self.model.predict(heads, relations, all_entities)

                        else:  # å®ä½“æ•°é‡è¾ƒå¤šæ—¶ï¼Œåˆ†å—è®¡ç®—
                            chunk_size = 1000
                            scores = torch.zeros(self.model.num_entities, device=self.device)

                            for chunk_start in range(0, self.model.num_entities, chunk_size):
                                chunk_end = min(chunk_start + chunk_size, self.model.num_entities)
                                chunk_entities = torch.arange(chunk_start, chunk_end, device=self.device)
                                chunk_heads = torch.full((len(chunk_entities),), h, device=self.device)
                                chunk_relations = torch.full((len(chunk_entities),), r, device=self.device)

                                chunk_scores = self.model.predict(chunk_heads, chunk_relations, chunk_entities)
                                scores[chunk_start:chunk_end] = chunk_scores

                        # è¿‡æ»¤æ‰å…¶ä»–å·²çŸ¥æ­£æ ·æœ¬
                        # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæé«˜æ•ˆç‡
                        for entity_id in range(self.model.num_entities):
                            if entity_id != t and (h, r, entity_id) in all_positive_triples:
                                scores[entity_id] = float('-inf')

                        # è·å–ç›®æ ‡å®ä½“çš„å¾—åˆ†
                        target_score = scores[t]

                        # è®¡ç®—æ’å
                        rank = torch.sum(scores > target_score).item() + 1

                        # å¤„ç†å¾—åˆ†ç›¸åŒçš„æƒ…å†µ
                        num_ties = torch.sum(scores == target_score).item()
                        if num_ties > 1:
                            rank = rank + (num_ties - 1) / 2.0

                        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                        total_reciprocal_rank += 1.0 / rank
                        total_samples += 1

                        for k in [1, 3, 10]:
                            if rank <= k:
                                hit_counts[k] += 1

                    # å®šæœŸæ‰“å°è¿›åº¦ï¼ˆå¯é€‰ï¼‰
                    if total_samples % 100 == 0 and total_samples > 0:
                        current_mrr = total_reciprocal_rank / total_samples
                        logger.debug(f"éªŒè¯è¿›åº¦: {total_samples} æ ·æœ¬, å½“å‰MRR: {current_mrr:.4f}")

                # é™åˆ¶éªŒè¯æ ·æœ¬æ•°é‡
                if total_samples >= getattr(self.config, 'max_valid_samples', 1000):
                    break

        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        if total_samples == 0:
            return {'MRR': 0.0, 'Hit@1': 0.0, 'Hit@3': 0.0, 'Hit@10': 0.0}

        metrics = {
            'MRR': total_reciprocal_rank / total_samples,
            'Hit@1': hit_counts[1] / total_samples,
            'Hit@3': hit_counts[3] / total_samples,
            'Hit@10': hit_counts[10] / total_samples,
            'num_evaluated': total_samples
        }

        return metrics

    def _evaluate_on_all_test_sets_with_weighted_avg(self, current_snapshot_id):
        """åœ¨æ‰€æœ‰ä¹‹å‰çš„æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶è®¡ç®—åŠ æƒå¹³å‡"""
        logger.info(f"\nğŸ“Š åœ¨æ‰€æœ‰æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆå¿«ç…§ 0 åˆ° {current_snapshot_id}ï¼‰")
        logger.info("=" * 60)

        all_test_results = {}
        total_mrr = 0.0
        total_hit1 = 0.0
        total_hit3 = 0.0
        total_hit10 = 0.0
        total_samples = 0

        # ç”¨äºå­˜å‚¨æ¯ä¸ªå¿«ç…§çš„æ ·æœ¬æ•°é‡ï¼ˆç”¨äºåŠ æƒï¼‰
        snapshot_weights = []
        snapshot_metrics = []

        for test_snapshot_id in range(current_snapshot_id + 1):
            test_loader = self.data_loader.get_snapshot_dataloader(
                test_snapshot_id,
                mode='test',
                batch_size=self.config.batch_size,
                shuffle=False
            )

            # ä½¿ç”¨ä¸¥æ ¼çš„è¿‡æ»¤è¯„ä¼°
            test_metrics = self._validate_with_filtering_parallel(test_loader, current_snapshot_id)
            test_dataset = self.data_loader.get_snapshot_dataset(test_snapshot_id, mode='test')
            num_test_samples = len(test_dataset)

            all_test_results[f'snapshot_{test_snapshot_id}'] = test_metrics
            all_test_results[f'snapshot_{test_snapshot_id}_samples'] = num_test_samples

            # ç´¯ç§¯åŠ æƒç»Ÿè®¡
            weight = num_test_samples
            snapshot_weights.append(weight)
            snapshot_metrics.append(test_metrics)

            total_mrr += test_metrics['MRR'] * weight
            total_hit1 += test_metrics['Hit@1'] * weight
            total_hit3 += test_metrics['Hit@3'] * weight
            total_hit10 += test_metrics['Hit@10'] * weight
            total_samples += weight

            logger.info(f"å¿«ç…§ {test_snapshot_id:2d} ({num_test_samples:4d} æ ·æœ¬): "
                        f"MRR={test_metrics['MRR']:.4f} "
                        f"Hit@1={test_metrics['Hit@1']:.4f} "
                        f"Hit@3={test_metrics['Hit@3']:.4f} "
                        f"Hit@10={test_metrics['Hit@10']:.4f}")

        # è®¡ç®—åŠ æƒå¹³å‡
        if total_samples > 0:
            weighted_avg_metrics = {
                'weighted_avg_MRR': total_mrr / total_samples,
                'weighted_avg_Hit@1': total_hit1 / total_samples,
                'weighted_avg_Hit@3': total_hit3 / total_samples,
                'weighted_avg_Hit@10': total_hit10 / total_samples
            }
        else:
            weighted_avg_metrics = {
                'weighted_avg_MRR': 0.0,
                'weighted_avg_Hit@1': 0.0,
                'weighted_avg_Hit@3': 0.0,
                'weighted_avg_Hit@10': 0.0
            }

        # è®¡ç®—ç®€å•å¹³å‡ï¼ˆä¸åŠ æƒï¼‰
        if len(snapshot_metrics) > 0:
            simple_avg_metrics = {
                'simple_avg_MRR': sum(m['MRR'] for m in snapshot_metrics) / len(snapshot_metrics),
                'simple_avg_Hit@1': sum(m['Hit@1'] for m in snapshot_metrics) / len(snapshot_metrics),
                'simple_avg_Hit@3': sum(m['Hit@3'] for m in snapshot_metrics) / len(snapshot_metrics),
                'simple_avg_Hit@10': sum(m['Hit@10'] for m in snapshot_metrics) / len(snapshot_metrics)
            }
        else:
            simple_avg_metrics = {
                'simple_avg_MRR': 0.0,
                'simple_avg_Hit@1': 0.0,
                'simple_avg_Hit@3': 0.0,
                'simple_avg_Hit@10': 0.0
            }

        all_test_results.update(weighted_avg_metrics)
        all_test_results.update(simple_avg_metrics)
        all_test_results['total_test_samples'] = total_samples

        logger.info("-" * 60)
        logger.info(f"ğŸ“ˆ åŠ æƒå¹³å‡æ€§èƒ½ (åŸºäºæ ·æœ¬æ•°é‡æƒé‡ï¼Œä½¿ç”¨è´Ÿè¿‡æ»¤):")
        logger.info(f"   MRR: {weighted_avg_metrics['weighted_avg_MRR']:.4f}")
        logger.info(f"   Hit@1: {weighted_avg_metrics['weighted_avg_Hit@1']:.4f}")
        logger.info(f"   Hit@3: {weighted_avg_metrics['weighted_avg_Hit@3']:.4f}")
        logger.info(f"   Hit@10: {weighted_avg_metrics['weighted_avg_Hit@10']:.4f}")

        logger.info(f"ğŸ“Š ç®€å•å¹³å‡æ€§èƒ½ (ç­‰æƒé‡):")
        logger.info(f"   MRR: {simple_avg_metrics['simple_avg_MRR']:.4f}")
        logger.info(f"   Hit@1: {simple_avg_metrics['simple_avg_Hit@1']:.4f}")
        logger.info(f"   Hit@3: {simple_avg_metrics['simple_avg_Hit@3']:.4f}")
        logger.info(f"   Hit@10: {simple_avg_metrics['simple_avg_Hit@10']:.4f}")

        logger.info(f"ğŸ“‹ æ€»æµ‹è¯•æ ·æœ¬: {total_samples}")
        logger.info("=" * 60)

        return all_test_results

    def diagnose_data_leakage(self):
        """è¯Šæ–­æ•°æ®æ˜¯å¦å­˜åœ¨æ³„éœ²é—®é¢˜"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ” æ•°æ®æ³„éœ²è¯Šæ–­æŠ¥å‘Š")
        logger.info("=" * 80)

        issues_found = []
        warnings_found = []

        for snapshot_id in range(len(self.data_loader.snapshots)):
            snapshot_data = self.data_loader.snapshots[snapshot_id]

            # æ£€æŸ¥1: éªŒè¯/æµ‹è¯•æ•°æ®æ˜¯å¦ä¸ºç©ºæˆ–æ¥è‡ªè®­ç»ƒæ•°æ®
            train_triples = set(tuple(t) for t in snapshot_data.get('train_triples', []))
            valid_triples = set(tuple(t) for t in snapshot_data.get('valid_triples', []))
            test_triples = set(tuple(t) for t in snapshot_data.get('test_triples', []))

            # éªŒè¯æ•°æ®æ£€æŸ¥
            if not valid_triples:
                warnings_found.append(f"å¿«ç…§ {snapshot_id}: æ²¡æœ‰éªŒè¯æ•°æ®")
            else:
                overlap = train_triples & valid_triples
                if overlap:
                    issues_found.append(f"å¿«ç…§ {snapshot_id}: éªŒè¯æ•°æ®ä¸è®­ç»ƒæ•°æ®é‡å  ({len(overlap)} ä¸ªä¸‰å…ƒç»„)")

            # æµ‹è¯•æ•°æ®æ£€æŸ¥
            if not test_triples:
                warnings_found.append(f"å¿«ç…§ {snapshot_id}: æ²¡æœ‰æµ‹è¯•æ•°æ®")
            else:
                overlap = train_triples & test_triples
                if overlap:
                    issues_found.append(f"å¿«ç…§ {snapshot_id}: æµ‹è¯•æ•°æ®ä¸è®­ç»ƒæ•°æ®é‡å  ({len(overlap)} ä¸ªä¸‰å…ƒç»„)")

                # éªŒè¯å’Œæµ‹è¯•æ•°æ®é‡å æ£€æŸ¥
                if valid_triples:
                    val_test_overlap = valid_triples & test_triples
                    if val_test_overlap:
                        issues_found.append(
                            f"å¿«ç…§ {snapshot_id}: éªŒè¯æ•°æ®ä¸æµ‹è¯•æ•°æ®é‡å  ({len(val_test_overlap)} ä¸ªä¸‰å…ƒç»„)")

            # æ£€æŸ¥2: æ•°æ®é›†å¤§å°åˆç†æ€§
            train_size = len(train_triples)
            valid_size = len(valid_triples)
            test_size = len(test_triples)

            if valid_size > 0 and valid_size < train_size * 0.05:
                warnings_found.append(f"å¿«ç…§ {snapshot_id}: éªŒè¯é›†è¿‡å° ({valid_size} vs {train_size} è®­ç»ƒæ ·æœ¬)")

            if test_size > 0 and test_size < train_size * 0.05:
                warnings_found.append(f"å¿«ç…§ {snapshot_id}: æµ‹è¯•é›†è¿‡å° ({test_size} vs {train_size} è®­ç»ƒæ ·æœ¬)")

            # æ£€æŸ¥3: å®ä½“/å…³ç³»èŒƒå›´
            entities_in_snapshot = set()
            relations_in_snapshot = set()

            for triple_set in [train_triples, valid_triples, test_triples]:
                for h, r, t in triple_set:
                    entities_in_snapshot.add(h)
                    entities_in_snapshot.add(t)
                    relations_in_snapshot.add(r)

            max_entity = max(entities_in_snapshot) if entities_in_snapshot else 0
            max_relation = max(relations_in_snapshot) if relations_in_snapshot else 0

            if max_entity >= self.data_loader.num_entities:
                issues_found.append(
                    f"å¿«ç…§ {snapshot_id}: å®ä½“IDè¶…å‡ºèŒƒå›´ ({max_entity} >= {self.data_loader.num_entities})")

            if max_relation >= self.data_loader.num_relations:
                issues_found.append(
                    f"å¿«ç…§ {snapshot_id}: å…³ç³»IDè¶…å‡ºèŒƒå›´ ({max_relation} >= {self.data_loader.num_relations})")

        # æ‰“å°è¯Šæ–­ç»“æœ
        if issues_found:
            logger.error("âŒ å‘ç°ä¸¥é‡é—®é¢˜:")
            for issue in issues_found:
                logger.error(f"   â€¢ {issue}")
        else:
            logger.info("âœ… æœªå‘ç°ä¸¥é‡çš„æ•°æ®æ³„éœ²é—®é¢˜")

        if warnings_found:
            logger.warning("âš ï¸  å‘ç°æ½œåœ¨é—®é¢˜:")
            for warning in warnings_found:
                logger.warning(f"   â€¢ {warning}")

        # å»ºè®®
        logger.info("\nğŸ“‹ å»ºè®®:")
        if not valid_triples or not test_triples:
            logger.info("   â€¢ ä½¿ç”¨çœŸå®çš„éªŒè¯/æµ‹è¯•æ•°æ®ï¼Œé¿å…ä»è®­ç»ƒæ•°æ®ä¸­åˆ’åˆ†")

        if issues_found:
            logger.info("   â€¢ é‡æ–°é¢„å¤„ç†æ•°æ®ï¼Œç¡®ä¿æ•°æ®é›†ä¹‹é—´æ²¡æœ‰é‡å ")

        logger.info("=" * 80)

        return len(issues_found) == 0

    def _load_best_model(self, snapshot_id):
        """åŠ è½½æœ€ä½³æ¨¡å‹"""
        model_path = os.path.join(self.config.checkpoint_dir, f'best_model_snapshot_{snapshot_id}.pth')
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            logger.info(f"å·²åŠ è½½å¿«ç…§ {snapshot_id} çš„æœ€ä½³æ¨¡å‹: {model_path}")
            return True
        else:
            logger.warning(f"æ‰¾ä¸åˆ°å¿«ç…§ {snapshot_id} çš„æœ€ä½³æ¨¡å‹: {model_path}")
            return False

    def _save_temp_best_model(self, snapshot_id):
        """ä¿å­˜ä¸´æ—¶æœ€ä½³æ¨¡å‹"""
        temp_path = os.path.join(self.config.checkpoint_dir, f'temp_best_snapshot_{snapshot_id}.pth')
        torch.save(self.model.state_dict(), temp_path)

    def _load_temp_best_model(self, snapshot_id):
        """åŠ è½½ä¸´æ—¶æœ€ä½³æ¨¡å‹"""
        temp_path = os.path.join(self.config.checkpoint_dir, f'temp_best_snapshot_{snapshot_id}.pth')
        if os.path.exists(temp_path):
            self.model.load_state_dict(torch.load(temp_path, map_location=self.device))

    def _save_best_model(self, snapshot_id):
        """ä¿å­˜æœ€ä½³æ¨¡å‹"""
        model_path = os.path.join(self.config.checkpoint_dir, f'best_model_snapshot_{snapshot_id}.pth')

        # ä¿å­˜å®Œæ•´çš„æ£€æŸ¥ç‚¹
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'snapshot_id': snapshot_id,
            'num_entities': self.model.num_entities,
            'num_relations': self.model.num_relations
        }

        torch.save(checkpoint, model_path)
        logger.info(f"æœ€ä½³æ¨¡å‹å·²ä¿å­˜: {model_path}")

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        temp_path = os.path.join(self.config.checkpoint_dir, f'temp_best_snapshot_{snapshot_id}.pth')
        if os.path.exists(temp_path):
            os.remove(temp_path)

    def _print_time_summary(self, total_time):
        """æ‰“å°æ—¶é—´ç»Ÿè®¡æ±‡æ€»"""
        logger.info("\n" + "=" * 80)
        logger.info("â±ï¸  æ—¶é—´ç»Ÿè®¡æ±‡æ€»")
        logger.info("=" * 80)

        # æ ¼å¼åŒ–æ—¶é—´æ˜¾ç¤ºå‡½æ•°
        def format_time(seconds):
            if seconds < 60:
                return f"{seconds:.2f}s"
            elif seconds < 3600:
                mins = int(seconds // 60)
                secs = seconds % 60
                return f"{mins}m {secs:.1f}s"
            else:
                hours = int(seconds // 3600)
                mins = int((seconds % 3600) // 60)
                secs = seconds % 60
                return f"{hours}h {mins}m {secs:.1f}s"

        # æ€»ä½“æ—¶é—´ç»Ÿè®¡
        logger.info(f"ğŸš€ æ€»æ‰§è¡Œæ—¶é—´: {format_time(total_time)}")
        logger.info(f"ğŸ“ æ€»è®­ç»ƒæ—¶é—´: {format_time(self.total_train_time)}")
        logger.info(f"ğŸ§ª æ€»æµ‹è¯•æ—¶é—´: {format_time(self.total_test_time)}")

        if total_time > 0:
            train_ratio = (self.total_train_time / total_time) * 100
            test_ratio = (self.total_test_time / total_time) * 100
            other_ratio = 100 - train_ratio - test_ratio

            logger.info(f"ğŸ“Š æ—¶é—´åˆ†å¸ƒ: è®­ç»ƒ {train_ratio:.1f}% | æµ‹è¯• {test_ratio:.1f}% | å…¶ä»– {other_ratio:.1f}%")

        # å„å¿«ç…§è¯¦ç»†æ—¶é—´
        if self.snapshot_train_times or self.snapshot_test_times:
            logger.info("\nğŸ“‹ å„å¿«ç…§è¯¦ç»†æ—¶é—´:")
            logger.info("-" * 60)
            logger.info(f"{'å¿«ç…§':<6} {'è®­ç»ƒæ—¶é—´':<15} {'æµ‹è¯•æ—¶é—´':<15} {'æ€»æ—¶é—´':<15}")
            logger.info("-" * 60)

            for i in range(max(len(self.snapshot_train_times), len(self.snapshot_test_times))):
                train_time = self.snapshot_train_times[i] if i < len(self.snapshot_train_times) else 0.0
                test_time = self.snapshot_test_times[i] if i < len(self.snapshot_test_times) else 0.0
                total_snap_time = train_time + test_time

                train_str = "è·³è¿‡" if train_time == 0.0 else format_time(train_time)

                logger.info(f"{i:<6} {train_str:<15} {format_time(test_time):<15} {format_time(total_snap_time):<15}")

            logger.info("-" * 60)

            # å¹³å‡æ—¶é—´ç»Ÿè®¡
            trained_snapshots = [t for t in self.snapshot_train_times if t > 0]
            if trained_snapshots:
                avg_train_time = sum(trained_snapshots) / len(trained_snapshots)
                logger.info(f"ğŸ“ˆ å¹³å‡è®­ç»ƒæ—¶é—´ (å·²è®­ç»ƒå¿«ç…§): {format_time(avg_train_time)}")

            if self.snapshot_test_times:
                avg_test_time = sum(self.snapshot_test_times) / len(self.snapshot_test_times)
                logger.info(f"ğŸ§ª å¹³å‡æµ‹è¯•æ—¶é—´: {format_time(avg_test_time)}")

        logger.info("=" * 80)

    def _print_final_summary(self, all_test_results):
        """æ‰“å°æœ€ç»ˆæ€§èƒ½æ€»ç»“ï¼ˆä¼˜åŒ–çš„è¡¨æ ¼æ ¼å¼ï¼‰"""
        logger.info("\n" + "=" * 90)
        logger.info("ğŸ¯ æœ€ç»ˆæ€§èƒ½æ€»ç»“ï¼ˆä½¿ç”¨è´Ÿè¿‡æ»¤è¯„ä¼°ï¼‰")
        logger.info("=" * 90)

        # è·å–æœ€åä¸€æ¬¡è¯„ä¼°çš„ç»“æœ
        last_evaluation = all_test_results[list(all_test_results.keys())[-1]]

        # æ”¶é›†å¿«ç…§æ•°æ®
        snapshot_data = []
        for snapshot_key, metrics in last_evaluation.items():
            if snapshot_key.startswith('snapshot_') and not snapshot_key.endswith('_samples'):
                snapshot_id = int(snapshot_key.split('_')[1])
                samples_key = f'snapshot_{snapshot_id}_samples'
                num_samples = last_evaluation.get(samples_key, 0)

                # è·å–è®­ç»ƒä¿¡æ¯
                training_info = "å·²è®­ç»ƒ"
                train_time = 0.0
                test_time = 0.0

                for history in self.training_history:
                    if history['snapshot_id'] == snapshot_id:
                        if history['training_results'].get('skipped', False):
                            training_info = "è·³è¿‡"
                        train_time = history.get('train_time', 0.0)
                        test_time = history.get('test_time', 0.0)
                        break

                snapshot_data.append({
                    'id': snapshot_id,
                    'samples': num_samples,
                    'mrr': metrics['MRR'],
                    'hit1': metrics['Hit@1'],
                    'hit3': metrics['Hit@3'],
                    'hit10': metrics['Hit@10'],
                    'status': training_info,
                    'train_time': train_time,
                    'test_time': test_time
                })

        # æŒ‰å¿«ç…§IDæ’åº
        snapshot_data.sort(key=lambda x: x['id'])

        # æ‰“å°è¯¦ç»†è¡¨æ ¼
        logger.info("ğŸ“Š å„å¿«ç…§æ€§èƒ½è¯¦æƒ…:")
        logger.info("=" * 90)
        header = f"{'å¿«ç…§':<4} {'çŠ¶æ€':<6} {'æ ·æœ¬æ•°':<7} {'MRR':<8} {'Hit@1':<8} {'Hit@3':<8} {'Hit@10':<8} {'è®­ç»ƒæ—¶é—´':<10} {'æµ‹è¯•æ—¶é—´':<10}"
        logger.info(header)
        logger.info("=" * 90)

        def format_time_short(seconds):
            if seconds == 0.0:
                return "è·³è¿‡"
            elif seconds < 60:
                return f"{seconds:.1f}s"
            elif seconds < 3600:
                return f"{int(seconds // 60)}m{int(seconds % 60)}s"
            else:
                return f"{int(seconds // 3600)}h{int((seconds % 3600) // 60)}m"

        for data in snapshot_data:
            train_time_str = format_time_short(data['train_time'])
            test_time_str = format_time_short(data['test_time'])

            row = (f"{data['id']:<4} {data['status']:<6} {data['samples']:<7} "
                   f"{data['mrr']:<8.4f} {data['hit1']:<8.4f} {data['hit3']:<8.4f} "
                   f"{data['hit10']:<8.4f} {train_time_str:<10} {test_time_str:<10}")
            logger.info(row)

        logger.info("=" * 90)
        # æ–°å¢ï¼š5ä¸ªå¿«ç…§åŠ æƒå‡å€¼æ±‡æ€»è¡¨
        logger.info("ğŸ† 5ä¸ªå¿«ç…§åŠ æƒå‡å€¼æ±‡æ€»è¡¨:")
        logger.info("=" * 70)
        logger.info(f"{'è®­ç»ƒè½®æ¬¡':<10} {'åŠ æƒMRR':<12} {'åŠ æƒHit@1':<12} {'åŠ æƒHit@3':<12} {'åŠ æƒHit@10':<12}")
        logger.info("=" * 70)

        # éå†æ‰€æœ‰è®­ç»ƒè½®æ¬¡çš„ç»“æœ
        for round_key in sorted(all_test_results.keys()):
            if round_key.startswith('after_snapshot_'):
                snapshot_num = int(round_key.split('_')[-1])
                round_results = all_test_results[round_key]

                # è·å–è¯¥è½®æ¬¡çš„åŠ æƒå¹³å‡ç»“æœ
                weighted_mrr = round_results.get('weighted_avg_MRR', 0.0)
                weighted_hit1 = round_results.get('weighted_avg_Hit@1', 0.0)
                weighted_hit3 = round_results.get('weighted_avg_Hit@3', 0.0)
                weighted_hit10 = round_results.get('weighted_avg_Hit@10', 0.0)

                logger.info(f"å¿«ç…§ {snapshot_num:<5} {weighted_mrr:<12.4f} {weighted_hit1:<12.4f} "
                            f"{weighted_hit3:<12.4f} {weighted_hit10:<12.4f}")

        logger.info("=" * 70)
        # æ‰“å°æ±‡æ€»ç»Ÿè®¡
        logger.info("ğŸ“ˆ æ±‡æ€»ç»Ÿè®¡ (âš ï¸  æ³¨æ„ï¼šä½¿ç”¨äº†è´Ÿè¿‡æ»¤è¯„ä¼°ï¼Œç»“æœæ›´ä¸¥æ ¼):")
        logger.info("-" * 90)

        # æ€§èƒ½æ±‡æ€»
        if 'weighted_avg_MRR' in last_evaluation:
            logger.info("ğŸ† åŠ æƒå¹³å‡æ€§èƒ½ (æŒ‰æ ·æœ¬æ•°é‡åŠ æƒ):")
            logger.info(f"    MRR: {last_evaluation['weighted_avg_MRR']:.4f} | "
                        f"Hit@1: {last_evaluation['weighted_avg_Hit@1']:.4f} | "
                        f"Hit@3: {last_evaluation['weighted_avg_Hit@3']:.4f} | "
                        f"Hit@10: {last_evaluation['weighted_avg_Hit@10']:.4f}")

        if 'simple_avg_MRR' in last_evaluation:
            logger.info("ğŸ“Š ç®€å•å¹³å‡æ€§èƒ½ (ç­‰æƒé‡):")
            logger.info(f"    MRR: {last_evaluation['simple_avg_MRR']:.4f} | "
                        f"Hit@1: {last_evaluation['simple_avg_Hit@1']:.4f} | "
                        f"Hit@3: {last_evaluation['simple_avg_Hit@3']:.4f} | "
                        f"Hit@10: {last_evaluation['simple_avg_Hit@10']:.4f}")

        # æ•°æ®æ±‡æ€»
        total_samples = sum(data['samples'] for data in snapshot_data)
        trained_snapshots = len([d for d in snapshot_data if d['status'] == "å·²è®­ç»ƒ"])
        skipped_snapshots = len([d for d in snapshot_data if d['status'] == "è·³è¿‡"])

        logger.info(f"ğŸ“‹ æ•°æ®æ±‡æ€»:")
        logger.info(f"    æ€»å¿«ç…§æ•°: {len(snapshot_data)} | "
                    f"å·²è®­ç»ƒ: {trained_snapshots} | "
                    f"è·³è¿‡: {skipped_snapshots} | "
                    f"æ€»æ ·æœ¬æ•°: {total_samples:,}")

        # é‡è¦æç¤º
        logger.info("\nâš ï¸  é‡è¦è¯´æ˜:")
        logger.info("   â€¢ æ­¤æ¬¡è¯„ä¼°ä½¿ç”¨äº†è´Ÿè¿‡æ»¤ï¼Œè¿‡æ»¤æ‰äº†å·²çŸ¥çš„æ­£æ ·æœ¬ä¸‰å…ƒç»„")

        logger.info("=" * 90)

    def _save_final_results(self, all_test_results, total_time):
        """ä¿å­˜æœ€ç»ˆç»“æœï¼ˆåŒ…å«æ—¶é—´ç»Ÿè®¡ï¼‰"""
        results_file = os.path.join(self.config.log_dir, 'final_results.json')

        final_results = {
            'training_history': self.training_history,
            'all_test_results': all_test_results,
            'time_statistics': {
                'total_time': total_time,
                'total_train_time': self.total_train_time,
                'total_test_time': self.total_test_time,
                'snapshot_train_times': self.snapshot_train_times,
                'snapshot_test_times': self.snapshot_test_times,
                'train_time_ratio': (self.total_train_time / total_time * 100) if total_time > 0 else 0,
                'test_time_ratio': (self.total_test_time / total_time * 100) if total_time > 0 else 0
            },
            'config': vars(self.config)
        }

        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2)

        logger.info(f"ğŸ’¾ æœ€ç»ˆç»“æœå·²ä¿å­˜: {results_file}")

        # æ‰“å°æœ€ç»ˆæ€§èƒ½æ€»ç»“
        self._print_final_summary(all_test_results)


def create_trainer(config, data_loader, device='cuda'):
    """åˆ›å»ºç®€åŒ–çš„æŒç»­å­¦ä¹ è®­ç»ƒå™¨"""
    return ContinualTrainer(config, data_loader, device)
